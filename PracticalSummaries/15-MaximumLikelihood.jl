# Practical Summary: Ch. 15, Maximum likelihood

# First, let's work with a simple one parameter model,
# for which we can get analytic results.

## Draw some data from the exponential distribution
using Distributions, Econometrics, Plots
Œ∏‚Å∞ = 4.0
n = 100
y = rand(Exponential(Œ∏‚Å∞),n)
histogram(y, normalize=true)
plot!(Exponential(Œ∏‚Å∞))

## Here are the ML results, for reference
using Econometrics, Distributions
log‚Ñí·µ¢ = Œ∏ -> logpdf.(Exponential(Œ∏[1]),y)
Œ∏start = [1.]
Œ∏hat = mleresults(log‚Ñí·µ¢, Œ∏start, y)[1]

##
# for this model, you should be able to work out, analytically,
# that the ML estimator is simply the sample mean. To confirm that:
mean(y)

## Let's get the results from basic theory: first the maximizer of log likelihood
using Optim
tol = 1e-08
s = Œ∏ -> -mean(log‚Ñí·µ¢(Œ∏))  # negative average log‚Ñí
Œ∏hat = Optim.optimize(s, Œ∏start, LBFGS(), 
                            Optim.Options(
                            g_tol = tol,
                            x_tol=tol,
                            f_tol=tol);autodiff=:forward).minimizer

## Now get the t-stats, from ‚Ñêhat and ùí•  hat.
using ForwardDiff
sc =  ForwardDiff.jacobian(log‚Ñí·µ¢, Œ∏hat) # get the score contributions
‚Ñêhat = mean(sc.^2.)
ùí•hat = -ForwardDiff.hessian(s, Œ∏hat)
# when you set n large (above), you should see that ‚Ñêhat ‚©¨ -ùí•hat  
# also, note that ùí•hat = 1/Œ∏hat¬≤, which is a result we can get analytically

# three forms of estimated variance
V1 = inv(‚Ñêhat)/n
se1 = sqrt(V1)
V2 = inv(-ùí•hat)/n
se2 = sqrt(V2)
V3 =  inv(ùí•hat)*‚Ñêhat*inv(ùí•hat)/n
se3 = sqrt(V3)
[se1 se2 se3]   # note that the estimators are a little different from one another
                # the last one, sandwich, is what's reported in mleresults.
##

# now let's work with a model that has regressors
using Distributions, Econometrics, Plots
n = 1000
x = [ones(n) randn(n) rand(n)]
Œ≤‚Å∞ = [-0.5, 1., 1.]
Œ∏ = Œ≤ ->  exp.(x*Œ≤)
y = rand.(Exponential.(Œ∏(Œ≤‚Å∞))) # independent, non-identically distributed observations

## Here's the exponential (correct) model
using Econometrics, Distributions, ForwardDiff, Calculus
log‚Ñí·µ¢ = Œ≤ -> [logpdf(Exponential(exp(x[i,:]'*Œ≤)),y[i]) for i = 1:n]
s(b) = mean(log‚Ñí·µ¢(b))
Œ≤start = zeros(3)
Œ≤hat, objval, junk = mleresults(log‚Ñí·µ¢, Œ≤start, y)
sc =  ForwardDiff.jacobian(log‚Ñí·µ¢, Œ≤hat) # get the score contributions
‚Ñêhat = zeros(3,3)
for i = 1:n
	‚Ñêhat .+= sc[i,:]*sc[i,:]'
end
‚Ñêhat ./= n
ùí•hat = Calculus.hessian(s, Œ≤hat, :central)
‚Ñêhat+ùí•hat # pretty close to zeros, as information matrix equality tells us

## Likelihood ratio test
using Distributions, Econometrics
# a true restriction
R = [1 1 1]
r = 1.5
obj(b) = -s(b) # need to minimize
Œ≤hatR, objvalR, junk = fmincon(obj, Œ≤hat, R, r)
objvalR = -objvalR # back to maximization
LR = 2*n*(objval - objvalR)
pval = 1. - cdf(Chisq(1), LR)
println("LR test: $R*Œ≤=$r   LR stat: $LR. p-value: $pval")
# a false restriction
R = [0 1 -1]
r = 1.
obj(b) = -s(b)
Œ≤hatR, objvalR, junk = fmincon(obj, Œ≤hat, R, r)
objvalR = -objvalR # back to maximization
LR = 2*n*(objval - objvalR)
pval = 1. - cdf(Chisq(1), LR)
println("LR test: $R*Œ≤=$r   LR stat: $LR. p-value: $pval")

## What about trying a œá¬≤ model? If we don't know the true density, we don't know which to use
log‚Ñí·µ¢ = Œ≤ -> [logpdf(Chisq(exp(x[i,:]'*Œ≤)),y[i]) for i = 1:n]
Œ≤start = zeros(3)
log‚Ñí·µ¢(zeros(3))
Œ≤hat = mleresults(log‚Ñí·µ¢, Œ≤start, y)[1]
sc =  ForwardDiff.jacobian(log‚Ñí·µ¢, Œ≤hat) # get the score contributions
‚Ñêhat = zeros(3,3)
for i = 1:n
	‚Ñêhat .+= sc[i,:]*sc[i,:]'
end
‚Ñêhat ./= n
ùí•hat = Calculus.hessian(s, Œ≤hat, :central)
‚Ñêhat+ùí•hat # a lot farther away from zeros, at least when n is large.

# How can we tell which model is best? The information criteria
# favor the exponential model, clearly. Comparing I and J,
# the information matrix equality holds much better for the
# exponential model than for the Chi-square. The IM test formalizes
# this, but here, we can see the intuitive basis for the test.


